{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython2",
  "version": 2,
  "kernelspec": {
   "name": "python37764bit19b558fa8aff4326bd1b6482cc6b6618",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning and Neural Networks with Keras\n",
    "\n",
    "> Notes from [this YouTube Series](https://www.youtube.com/watch?v=zYnI4iWRmpc), Full course notes can be found on [GitHub](https://github.com/jeffheaton/t81_558_deep_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Neural Networks\n",
    "\n",
    "A Neural Network takes the some kind of data and has the ability to handle and process data that other ML models are not really able to process\n",
    "\n",
    "In a normal model you would pass in a 1D vector such as a list of predictors, with a an NN you can pass in more complex data and the model will place weight on the position as well as the values of a respective data point which is something other models can't necessarily handle\n",
    "\n",
    "Some examples of higher order data can be:\n",
    "\n",
    "1. 1D Vector - Normal input, like a row in a spreadsheet\n",
    "2. 2D Matrix - Grayscale image\n",
    "3. 3D Matrix - Colour image\n",
    "4. nD Matrix - Any higher order data\n",
    "\n",
    "With traditional models we speak about regression or classification. \n",
    "\n",
    "A regression network could have a single numerical output, or a classification network could have a set of potential binary outputs for each classes (like one-hot) or a probability of the result being each of the possible outputs\n",
    "\n",
    "Neural Networks are also capble of more complex outputs or even combinations of outputs\n",
    "\n",
    "In general an NN consists of an Input Layer which takes in the input data, a few hidden layers which proces the data, and an output layer which is our target outcome. Each layer passes a weighted data to each model\n",
    "\n",
    "There are usually these types of neurons:\n",
    "\n",
    "1. Input - get the input data\n",
    "2. Hidden - between input and output and abstract processing\n",
    "3. Output - the output that's calculated\n",
    "4. Context - hold state between calls to the network\n",
    "5. Bias Neurons - similar to a y-intercept, alow us to offset the data to a neurons\n",
    "\n",
    "\n",
    "Neural networks pass data to nodes using Activation functions, some common ones are:\n",
    "\n",
    "- Rectified Linear Unit (ReLU) - used for hidden layers\n",
    "- Softmax - output for classification\n",
    "- Linear - for regression\n",
    "\n",
    "The Bias Neuron along with a Weight allow us to move and scale our activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow and Keras\n",
    "\n",
    "TensorFlow is the low-level library for Neural Networks, and Keras is an API that sits on top of TF and allows you to interact with it at a higher level\n",
    "\n",
    "> The current version of TF requires Python 3.7, so just align with that\n",
    "\n",
    "TensorBoard is a way to visualize Neural Networks \n",
    "\n",
    "### Using Tensorflow Directly\n",
    "\n",
    "#### Simple Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor([[12.]], shape=(1, 1), dtype=float32)\n"
    },
    {
     "data": {
      "text/plain": "12.0"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix1 = tf.constant([[3., 3.]])\n",
    "\n",
    "matrix2 = tf.constant([[2.], [2.]])\n",
    "\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "\n",
    "print(product)\n",
    "float(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Variables\n",
    "\n",
    "Variables can be created, used, and resasigned and recalculated with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[-2. -1.]\n"
    }
   ],
   "source": [
    "x = tf.Variable([1., 2.])\n",
    "a = tf.constant([3., 3.])\n",
    "\n",
    "print(tf.subtract(x, a).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1. 3.]\n"
    }
   ],
   "source": [
    "x.assign([4., 6.])\n",
    "\n",
    "print(tf.subtract(x, a).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Keras with MPG Dataset\n",
    "\n",
    "Keras enables us to think about the Layers in an NN, we'll use the Miles Per Gallon dataset which uses the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "\n",
    "COLUMN_NAMES = [\n",
    "        'mpg', \n",
    "        'cylinders', \n",
    "        'displacement', \n",
    "        'horsepower', \n",
    "        'weight', \n",
    "        'acceleration', \n",
    "        'model year', \n",
    "        'origin', \n",
    "        'car name'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "df = pd.read_fwf(\n",
    "    DATA_URL, \n",
    "    names=COLUMN_NAMES,\n",
    "    na_values=['NA', '?']\n",
    ")\n",
    "\n",
    "# fill missing\n",
    "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mpg</th>\n      <th>cylinders</th>\n      <th>displacement</th>\n      <th>horsepower</th>\n      <th>weight</th>\n      <th>acceleration</th>\n      <th>model year</th>\n      <th>origin</th>\n      <th>car name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18.0</td>\n      <td>8</td>\n      <td>307.0</td>\n      <td>130.0</td>\n      <td>3504.0</td>\n      <td>12.0</td>\n      <td>70</td>\n      <td>1</td>\n      <td>\"chevrolet chevelle malibu\"</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>15.0</td>\n      <td>8</td>\n      <td>350.0</td>\n      <td>165.0</td>\n      <td>3693.0</td>\n      <td>11.5</td>\n      <td>70</td>\n      <td>1</td>\n      <td>\"buick skylark 320\"</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18.0</td>\n      <td>8</td>\n      <td>318.0</td>\n      <td>150.0</td>\n      <td>3436.0</td>\n      <td>11.0</td>\n      <td>70</td>\n      <td>1</td>\n      <td>\"plymouth satellite\"</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16.0</td>\n      <td>8</td>\n      <td>304.0</td>\n      <td>150.0</td>\n      <td>3433.0</td>\n      <td>12.0</td>\n      <td>70</td>\n      <td>1</td>\n      <td>\"amc rebel sst\"</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>17.0</td>\n      <td>8</td>\n      <td>302.0</td>\n      <td>140.0</td>\n      <td>3449.0</td>\n      <td>10.5</td>\n      <td>70</td>\n      <td>1</td>\n      <td>\"ford torino\"</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "    mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n0  18.0          8         307.0       130.0  3504.0          12.0   \n1  15.0          8         350.0       165.0  3693.0          11.5   \n2  18.0          8         318.0       150.0  3436.0          11.0   \n3  16.0          8         304.0       150.0  3433.0          12.0   \n4  17.0          8         302.0       140.0  3449.0          10.5   \n\n   model year  origin                     car name  \n0          70       1  \"chevrolet chevelle malibu\"  \n1          70       1          \"buick skylark 320\"  \n2          70       1         \"plymouth satellite\"  \n3          70       1              \"amc rebel sst\"  \n4          70       1                \"ford torino\"  "
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['mpg', 'car name'], axis=1).values\n",
    "y = df[['mpg']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Regression Model with Keras\n",
    "\n",
    "When building a Neural Network we take the following steps:\n",
    "\n",
    "1. Create a Sequential\n",
    "2. Define the Hidden Layers\n",
    "3. Define the Output Layer\n",
    "4. Compile and Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define Hidden Layers\n",
    "\n",
    "Define the first hiddel layer with the `input_dim` to be the shape of our input data set (`X` columns in this case)\n",
    "\n",
    "> A dense layer is one where each neuron is connected to the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(25, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define the Output Layer\n",
    "\n",
    "This is depends on the dimensionality of the output, similar to the input. For this case it is one dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Compile and train the model\n",
    "\n",
    "We specify a `loss` function and an `optimizer` for the model, and then give it the `X` and `y` values to train on a well as how many `epoch`s we want it to train for\n",
    "\n",
    "For a Regression NN you usually use MSE as the loss\n",
    "\n",
    "> We can also make use of methods to increase the model's effectiveness and identifying the optimal number of epochh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2041a5ff688>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X, y, verbose=0, epochs=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'MSE: 3.4881811444565303'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X)\n",
    "score = np.sqrt(metrics.mean_squared_error(y_pred, y))\n",
    "'MSE: ' + str(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Classification Model with Keras\n",
    "\n",
    "Building a Classification Model is much the same, however we need to ensure that we hot-encode our categorical values, and in this case we'll have a categorical output which means more than one potential result\n",
    "\n",
    "For this we're making use of the [Iris Dataset](https://archive.ics.uci.edu/ml/datasets/Iris)\n",
    "\n",
    "However for a Multi-Class classification we use `softmax` and `categorical_crossentropy`\n",
    "\n",
    "For a Binary we can additionally use an appliccable loss and activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "\n",
    "COLUMN_NAMES = [\n",
    "   'sepal length',\n",
    "   'sepal width',\n",
    "   'petal length',\n",
    "   'petal width',\n",
    "   'class'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_URL, names=COLUMN_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length</th>\n      <th>sepal width</th>\n      <th>petal length</th>\n      <th>petal width</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   sepal length  sepal width  petal length  petal width        class\n0           5.1          3.5           1.4          0.2  Iris-setosa\n1           4.9          3.0           1.4          0.2  Iris-setosa\n2           4.7          3.2           1.3          0.2  Iris-setosa\n3           4.6          3.1           1.5          0.2  Iris-setosa\n4           5.0          3.6           1.4          0.2  Iris-setosa"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('class', axis=1).values\n",
    "dummies = pd.get_dummies(df['class'])\n",
    "\n",
    "species = dummies.columns\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2041b8c9c88>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, verbose=0, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 2 1\n 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nExpected: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nIndex(['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n       'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n       'Iris-setosa'],\n      dtype='object')\n"
    },
    {
     "data": {
      "text/plain": "'Accuracy: 0.9733333333333334'"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X)\n",
    "\n",
    "predict_classes = np.argmax(y_pred,axis=1)\n",
    "expected_classes = np.argmax(y,axis=1)\n",
    "print(f\"Predictions: {predict_classes}\")\n",
    "print(f\"Expected: {expected_classes}\")\n",
    "\n",
    "print(species[predict_classes[1:10]])\n",
    "\n",
    "score = metrics.accuracy_score(expected_classes,predict_classes)\n",
    "'Accuracy: ' + str(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Neural Networks\n",
    "\n",
    "We can store data in a few different formats, the ideal one is the `HDF5` format which stores the structure and weights for the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "\n",
    "We can save the model we just trained with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = './exported-models/iris-model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.engine.sequential.Sequential at 0x2041b8141c8>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = load_model(MODEL_SAVE_PATH)\n",
    "\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping to prevent Overfitting\n",
    "\n",
    "We can make use of test/train sets to help us prevent overfitting, this is done by helping us identify when to stop training the network\n",
    "\n",
    "It's important that we save our score at a good fitted value\n",
    "\n",
    "Data is usually split into the following sets:\n",
    "\n",
    "- Test \n",
    "- Train \n",
    "- Holdout\n",
    "\n",
    "If we have have a lot of data we can even try to have multiple test and train sets\n",
    "\n",
    "To train the model we'll do the normal preprocessing and model definition as before, and then we'll implement `EarlyStopping` from Keras when doing the `model.fit` portion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing and Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "\n",
    "COLUMN_NAMES = [\n",
    "   'sepal length',\n",
    "   'sepal width',\n",
    "   'petal length',\n",
    "   'petal width',\n",
    "   'class'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_URL, names=COLUMN_NAMES)\n",
    "\n",
    "X = df.drop('class', axis=1).values\n",
    "dummies = pd.get_dummies(df['class'])\n",
    "\n",
    "species = dummies.columns\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split (\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model\n",
    "\n",
    "> The below applies to both categorical and regression models\n",
    "\n",
    "We can train the model using an `EarlyStopping` callback, in this we specify:\n",
    "\n",
    "1. The metric we want to monitor for change, `val_loss` to use the validation `loss` we defined for the model as the metric\n",
    "2. The minimum change we want to for stability, this will not have much of an impact if made smaller\n",
    "3. The number of rounds we want the delta to be small for before stopping\n",
    "4. The mode, usually keep this at `auto` but it is whether to minimize or maximize the error\n",
    "5. Restore best weights automatically, always keep this at `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    min_delta=1e-3, \n",
    "    patience=50,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Restoring model weights from the end of the best epoch.\nEpoch 00075: early stopping\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x204254a7108>"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, y_train, \n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[monitor],\n",
    "    verbose=0,\n",
    "    epochs=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure the Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Accuracy: 0.9736842105263158'"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "predicted_classes = np.argmax(y_pred, axis=1)\n",
    "expected_classes = np.argmax(y_test, axis=1)\n",
    "score = metrics.accuracy_score(expected_classes, predicted_classes)\n",
    "\n",
    "'Accuracy: ' + str(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Vectors and Tabular Data\n",
    "\n",
    "All data that comes into a Neural Network must be numerical\n",
    "\n",
    "Some of the processing we will typically do are:\n",
    "\n",
    "1. Convert categorical values to dummies (features and target)\n",
    "2. Drop any columns like ID, etc.\n",
    "3. Get all the different numerical data to be in a the same range\n",
    "4. Center numerical data around a mean of zero\n",
    "5. Fill missing values as appropriate for the relevant data \n",
    "6. If we have missing data in the targe column we should drop those rows\n",
    "\n",
    "> We can use a Z-score to work with points 3 and 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLassification Metrics\n",
    "\n",
    "Sometimes we care about additional factors than just the accuracy, such as the counts of false positives or negatives etc.\n",
    "\n",
    "### ROC Values\n",
    "\n",
    "- Flase Positives\n",
    "- False Negatives\n",
    "- True Positives\n",
    "- True Negatives\n",
    "\n",
    "These can also be be described as Type-1 and Type-2 Erors as well as Test Sensitivity and Specificity\n",
    "\n",
    "A sensitive NN will lead to more false positives, and more specific NN will lead towards fewer false positives \n",
    "\n",
    "A ROC chart compares our model to random predictions, the higher up our line is the more accurate our model. We measure the area under this curve to get the AUC Value, if our model falls below the `0.5` mark (below the random line) it means our model is doing worse than a random guess (which is really bad)\n",
    "\n",
    "### Log Loss\n",
    "\n",
    "A Log Loss calculation we can get a sort of accuracy score that's more harsh on overconfidence\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "This compares our predicted values to the actual values, in this we would ideally want to see a strong diagonal correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics\n",
    "\n",
    "When working with regression models there are different metrics that we can use in order to \n",
    "\n",
    "### Mean Squared Error and Root Mean Squared Error\n",
    "\n",
    "We usually work with the MSE value which is sort of releative to our dataset, square rooting this gives us the RMSE which tells us how close we are to our actual value in the same units as our target data \n",
    "\n",
    "### Lift Chart\n",
    "\n",
    "A Lift chart is a way to compare our model output to the actual test data in order to see how our model compares over specific value ranges in the target vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "We have a few two types of backpropagation which we use when training a model\n",
    "\n",
    "1. Classic - using gradient descent (e.g. 0.1, 0.01, 0.001)\n",
    "2. Momentum - pushes weights in order to avoid local minumums (e.g. 0.9)\n",
    "3. Batch and Online - update weights in batches instead of every iteration\n",
    "4. Stochastic Gradient Descent - Often used with batching, network trained on differing sets of the data and decreases overfitting by focusing on a smaller number of weights\n",
    "\n",
    "Additionally we have a few methods that can help us to automate certain hyperparameters;\n",
    "\n",
    "- Resilient Propogation - uses only the gradient magnitude and allows each neuron it's own learning rate\n",
    "- Nesterov accelerated gradient - helps mitigate the risk of choosing a bad batch\n",
    "- Adagrad - allows an automatically decaying learning rate and momentum per weight \n",
    "- Adadelta - Based on on Adagrad, monotonically decreasing learning rate\n",
    "\n",
    "\n",
    "There are also other non gradient methods such as:\n",
    "\n",
    "- Simulated Annealing\n",
    "- Generic Algorithm\n",
    "- Particle Swarm Optimization\n",
    "- Nelder Mead\n",
    "\n",
    "> [Some interestnig diagrams](https://ruder.io/optimizing-gradient-descent/index.html#visualizationofalgorithms) comparing different algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Regularization is used to combat overfitting. The two types of Regularization we have Lasso (L1) and Ridge (L2) regularization\n",
    "\n",
    "L1 regularization can help a network focus on the important factors\n",
    "\n",
    "The `alpha` value lets us say how important the regularization is to our model, in general a higher `alpha` will cause the model to have a lower accuracy but prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso (L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n      normalize=False, positive=False, precompute=False, random_state=0,\n      selection='cyclic', tol=0.0001, warm_start=False)"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(random_state=0, alpha=0.1)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge (L2)\n",
    "\n",
    "L2 Regression (Ridge) lets us focus a bit less on the weightings than the L1 method and penalizes the model less for large weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=None,\n      normalize=False, random_state=0, solver='auto', tol=0.001)"
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(random_state=0, alpha=0.1)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet\n",
    "\n",
    "ElasticNet uses a combination of L1 and L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "ElasticNet(alpha=0.1, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n           max_iter=1000, normalize=False, positive=False, precompute=False,\n           random_state=0, selection='cyclic', tol=0.0001, warm_start=False)"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "model = ElasticNet(random_state=0, alpha=0.1)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Dropout is another method of regularization and is applied during training\n",
    "\n",
    "When using dropout we disable random neurons in each epoch to prevent them from becoming too specialized. This helps us to prevent overfitting as well as reduce the variance in the overall trained network\n",
    "\n",
    "The dropped neurons are re-added once the training is complete\n",
    "\n",
    "In order to use dropout in Keras we can add a `Dropout` layer with a value for what fraction of neurons we want to be dropped out\n",
    "\n",
    "> The suggestion is usually not to use a dropout after the final hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "\n",
    "COLUMN_NAMES = [\n",
    "   'sepal length',\n",
    "   'sepal width',\n",
    "   'petal length',\n",
    "   'petal width',\n",
    "   'class'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_URL, names=COLUMN_NAMES)\n",
    "\n",
    "X = df.drop('class', axis=1).values\n",
    "dummies = pd.get_dummies(df['class'])\n",
    "\n",
    "species = dummies.columns\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will train the model, the model has the following:\n",
    "\n",
    "1. 1st Dense Layer with 50 neurons and ReLU activation\n",
    "2. A dropout of 50%\n",
    "3. 2nd Dense Layer with 25 neurons, ReLU, and an L1 Regularization\n",
    "4. An Output Layer with the categories and Softmax activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(\n",
    "        25, \n",
    "        activation='relu',\n",
    "        activity_regularizer=regularizers.l1(1e-4)\n",
    "    ))\n",
    "\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1804e5a00c8>"
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=0,\n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n 2]\n[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n 1]\n"
    }
   ],
   "source": [
    "predicted_classes = np.argmax(y_pred, axis=1)\n",
    "expected_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(predicted_classes)\n",
    "print(expected_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Accuracy: 0.9736842105263158'"
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = metrics.accuracy_score(expected_classes, predicted_classes)\n",
    "\n",
    "'Accuracy: ' + str(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking and Regularization\n",
    "\n",
    "So far we've seen of a network is based on the following:\n",
    "\n",
    "- Number of layers\n",
    "- How many neurons per layers\n",
    "- Activation functions for each layers\n",
    "- Droppout per layer \n",
    "- L2 and L2 Regularization\n",
    "\n",
    "There are additional parameters that can also influence the network\n",
    "\n",
    "Due to the different parameters and the random nature of a network it can be difficult to see if our change in hyperparameters is actually impacting the output of a network\n",
    "\n",
    "We can do something called Bootstrapping which is similar to cross validation with replacement and early stopping to help our network average converge and after how many epochs this takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap\n",
    "\n",
    "For a Regression model we can use:\n",
    "\n",
    "```py\n",
    "boot = ShuffleSplit(n_splits=SPLITS, test_size=0.1)\n",
    "```\n",
    "\n",
    "and then:\n",
    "\n",
    "```py\n",
    "for train, test in boot.split(X):\n",
    "    # train model\n",
    "```\n",
    "\n",
    "However, for a Categorical classification we want to ensure that we have a class balance, we can do this with the `StratifiedShuffleSplit` which works like so:\n",
    "\n",
    "> Note that the `EarlyStopping` monitor returns `0` if the training was not early stopped (e.g. trained till end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tracker = []\n",
    "epoch_tracker = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, test in boot.split(X, y): # using the data from the last import\n",
    "\n",
    "    X_train = X[train]\n",
    "    X_test = X[test]\n",
    "    y_train = y[train]\n",
    "    y_test = y[test]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=X.shape[1], activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(\n",
    "            25, \n",
    "            activation='relu'\n",
    "        ))\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    monitor = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=1e-3,\n",
    "        patience=50,\n",
    "        verbose=0,\n",
    "        mode='auto',\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train, \n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[monitor],\n",
    "        verbose=0,\n",
    "        epochs=1000\n",
    "    )\n",
    "\n",
    "    epoch_tracker.append(\n",
    "        monitor.stopped_epoch if monitor.stopped_epoch > 0 else 1000\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    predicted_classes = np.argmax(y_pred, axis=1)\n",
    "    expected_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    score = metrics.accuracy_score(expected_classes, predicted_classes)\n",
    "\n",
    "    accuracy_tracker.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Score</th>\n      <th>Epochs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>15.000000</td>\n      <td>15.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.993333</td>\n      <td>350.200000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.013801</td>\n      <td>76.766064</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.966667</td>\n      <td>226.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.000000</td>\n      <td>308.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n      <td>321.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n      <td>397.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>528.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "           Score      Epochs\ncount  15.000000   15.000000\nmean    0.993333  350.200000\nstd     0.013801   76.766064\nmin     0.966667  226.000000\n25%     1.000000  308.000000\n50%     1.000000  321.000000\n75%     1.000000  397.000000\nmax     1.000000  528.000000"
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({ \n",
    "    \"Score\": accuracy_tracker,\n",
    "    \"Epochs\": epoch_tracker\n",
    "}).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}