{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Deep Learning with TensorFlow Module 3\n\n[Based on this CognitiveClass Course](https://cognitiveclass.ai/courses/deep-learning-tensorflow/)\n\n## Recurrent Neural Nets\n\n### Sequential Data\n\nSequential data produces a problem when trying to make use of traditional feed-forward neural networks\n\nNNs assume that inputs are independent and therefore do not take into consideration the impact of previous data\n\n### Recurrent Neural Networks\n\nRNNs are able to maintain a state or context which allows the model to know what was previously calculated. The model feeds back in previous inputs with the new input in order to help us predict sequential information\n\n\n$$\nh_{new}=tanh(W_h\\cdot h_{prev}+W_x\\cdot x)\n$$\n\nWhere $h_{prev}$ is the context, and the new state $h_{new}$ which will yield an output $y$\n\n$$\ny=W_y\\cdot h_{new}\n$$\n\nRNNs need to keep track of state at any given timepoint and can become computationally expensive. These are also very sensative to changes in parameters and can suffer from a Vanishing or Exploding gradient\n\nOverall these models can be more difficult to train\n\n### Long Short-Term Memory\n\nA standard RNN can be difficult when trying to learn very long sequences\n\nA solution to this is the LSTM model which maintains a strong gradient over relatively long sequences\n\nAn LSTM is contained of a memory cell and 3 logistic gates for writing, reading, and forgeting which control the flow of data\n\nManipulating these gates allows an LSTM to remember the appropriate level of information\n\nWe can also step LSTMs on top of each other to create hierarchial data representations\n\nDuring the training process the network will learn how much old, and new information to remember as well as the weights and biases based on the different levels of state for each gate at each layer\n\n### Language Modeling\n\nLanguage modeling is the process of assigning prbabilities to a sequence of words. We can make use of an RNN to form the context based on previous words and thereafter output the new word\n\nWord embedding is a way to process words by decoding a word to a spefici vector. The vectors for the vocabulary are initialized randomly and then are updated based on the context that the word is the same and hence words used in similar contexts \n\nWe will pass in batches of text and the network will output the likelihood that a specific word follows the input set, this will then allow us to compare the actual output and model output probabilities to help train the model through back-prop", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "### Lab - LSTM Basics\n\nAn LSTM makes use of a linear unit which is the information cell and is surrounded by the tree logistic gates which are responsible for maintaining data, the gates selectively:\n\n- Input/Write\n    - Handles writing data to the information cell\n- Output/Read\n    - Sends data back to the RNN\n- Keep/Remember\n    - Handles maintaining and modification of data in infomation cell\n    \nThe gates above are analog and multiplicative and modify the data based on the signal they receive\n\nThe usual flow of operations in an LSTM are as follows:\n\n1. The Keep gate decides whether to keep or forget memory currently in memory and receives the input and state of the RNN and passes it through a Sigmoid activation. If $k_t$ has a value of $1$ the data will be stored perfectly, $0$ means it will be forgotten completely. Given the incoming state $S_{t-1}$ and $x_1$ as the incoming input and $W_k$ and $B_k$ as the weights and biases for the keep gate and the previous state of memory $Old_t$ we have the following equations\n\n$$\nK_t = \\sigma(W_k\\times[S_{t-1},x_t]+B_k)\n$$\n\n\n$$\nOld_t=K_t\\times Old_{t-1}\n$$\n\nWe can see from this that the $Old_{t-1}$ is multiplied by the current $K_t$ value\n\n2. The input and state are passed to the Input Gate where there is another Sigmoid activation applied, the result of which is $I_t$. The result of processing the inputs by the RNN are $C_t$\n\n$$\nI_t = \\sigma(W_i\\times[S_{t-1},x_t]+B_i)\n$$\n\n\n$$\nNew_t=I_t\\times C_t\n$$\n\n$New$ is the data to be input to the memory cell and is added to whatever is already in the memory cell\n\n$$\nCell_t=Old_t+New_t\n$$\n\nThis is the candidate data to be kept in the memory cell We use the Output Gate to decide how much of the new output we will keep\n\n$$\nO_t=\\sigma(W_o\\times[S_{t-1},x_t]+b_o)\n$$\n\n$$\nOutput_t=O_t\\times tanh(Cell_t)\n$$", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Create an LSTM Network\n\nWe'll create a simple LSTM to understand the network architecture", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "IPython.notebook.set_autosave_interval(120000)"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Autosaving every 120 seconds\n"
                }
            ], 
            "source": "%autosave 120"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import numpy as np\nimport tensorflow as tf"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sess = tf.Session()"
        }, 
        {
            "source": "We want to create a network with 1 LSTM cell, this will have two inputs - `prv_output` an `prv_state` as inputs, also known as $h$ and $c$ respectively. In order to do this we initialize a `state` vector which is a tuple with 2 elements, each of size `1, 4` and will be made up of `prv_output` and `prv_state`", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "(<tf.Tensor 'zeros:0' shape=(1, 4) dtype=float32>, <tf.Tensor 'zeros:0' shape=(1, 4) dtype=float32>)\n"
                }
            ], 
            "source": "LSTM_CELL_SIZE = 4\n\nlstm_cell = tf.contrib.rnn.BasicLSTMCell(LSTM_CELL_SIZE, \n                                         state_is_tuple=True)\nstate = (tf.zeros([1, LSTM_CELL_SIZE]),)*2\nprint(state)"
        }, 
        {
            "source": "#### Define the Sample Input\n\nWe can define a sample output with a `batch_size = 1` and `seq_len = 6`", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[[ 3.  2.  2.  2.  2.  2.]]\n"
                }
            ], 
            "source": "sample_input = tf.constant([[3, 2, 2, 2, 2, 2]], dtype=tf.float32)\nprint(sess.run(sample_input))"
        }, 
        {
            "source": "#### Run the Sample Input\n\nWe can run the sample input by passing it to the `lstm_cell` and looking at the output", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "LSTMStateTuple(c=array([[-0.03636235,  0.83891821,  0.23306049, -0.55370402]], dtype=float32), h=array([[-0.00570551,  0.08357365,  0.04054929, -0.27129793]], dtype=float32))\n"
                }
            ], 
            "source": "with tf.variable_scope('LSTM_sample1'):\n    output, state_new = lstm_cell(sample_input, state)\nsess.run(tf.global_variables_initializer())\nprint(sess.run(state_new))"
        }, 
        {
            "source": "Based on the above we can see that we have a $c$ and $h$ value in the LSTM State. We can take a look at the ouput by evaluating it as follows", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[[-0.00570551  0.08357365  0.04054929 -0.27129793]]\n"
                }
            ], 
            "source": "print(sess.run(output))"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sess.close()"
        }, 
        {
            "source": "#### Stacked LSTM\n\nWe can create a two layer LSTM as follows", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sess = tf.Session()"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "input_dim = 6"
        }, 
        {
            "source": "Create multiple cells", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cells = []"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "LSTM_CELL_SIZE_1 = 4\ncell1 = tf.contrib.rnn.LSTMCell(LSTM_CELL_SIZE_1)\ncells.append(cell1)"
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "LSTM_CELL_SIZE_2 = 5\ncell2 = tf.contrib.rnn.LSTMCell(LSTM_CELL_SIZE_2)\ncells.append(cell2)"
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "stacked_lstm = tf.contrib.rnn.MultiRNNCell(cells)"
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "data = tf.placeholder(tf.float32, [None, None, input_dim])\noutput, state = tf.nn.dynamic_rnn(stacked_lstm, data, dtype=tf.float32)"
        }, 
        {
            "source": "Sample Input for this will be `2, 3, 6`", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 18, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[[[1, 2, 3, 4, 3, 2], [1, 2, 1, 1, 1, 2], [1, 2, 2, 2, 2, 2]],\n [[1, 2, 3, 4, 3, 2], [3, 2, 2, 1, 1, 2], [0, 0, 0, 0, 3, 2]]]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "#Batch size x time steps x features.\nsample_input = [[[1,2,3,4,3,2], [1,2,1,1,1,2],[1,2,2,2,2,2]],[[1,2,3,4,3,2],[3,2,2,1,1,2],[0,0,0,0,3,2]]]\nsample_input"
        }, 
        {
            "source": "#### Evaluate the Output", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Tensor(\"rnn/transpose:0\", shape=(?, ?, 5), dtype=float32)\n"
                }
            ], 
            "source": "print(output)"
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[[[-0.00457329  0.01386554  0.00200699 -0.00482764 -0.0085205 ]\n  [-0.00555866  0.02601253 -0.00947261 -0.00118891 -0.01125968]\n  [-0.00866533  0.04085833 -0.01180315  0.0016048  -0.01417257]]\n\n [[-0.00457329  0.01386554  0.00200699 -0.00482764 -0.0085205 ]\n  [-0.00063138  0.01153464 -0.00904999  0.00572045 -0.00064348]\n  [-0.01402552  0.02387906 -0.02340911  0.0096273  -0.00310375]]]\n"
                }
            ], 
            "source": "sess.run(tf.global_variables_initializer())\nprint(sess.run(output, feed_dict={data: sample_input}))"
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sess.close()"
        }, 
        {
            "source": "### Lab - LSTM with MNIST\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}